"""Threads Scraper (with total post count)

æ”¯æ´ï¼š
* å–®ä¸€å¸³è™Ÿæˆ–å¤šå¸³è™Ÿæ‰¹æ¬¡
* --json è¼¸å‡ºåŸå§‹ JSON
* é¡å¤–é¡¯ç¤ºæ¯å€‹å¸³è™Ÿçš„ **ç¸½è²¼æ–‡æ•¸**ï¼ˆè‹¥èƒ½å¾ JSON å–å¾—ï¼‰
"""

import asyncio
import json
from typing import List, Dict, Optional, Tuple
import jmespath
from nested_lookup import nested_lookup
from parsel import Selector
from playwright.async_api import async_playwright
from datetime import datetime,timedelta, timezone
import os
class Threads_scraper:
        def __init__(self,search_choice:str="like_count",acc:bool=False,username:Optional[List[str]]=None):
            try:
                with open('config/existID.json', 'r', encoding='utf-8') as f:
                    self.seen = set(json.load(f))
            except (FileNotFoundError, json.JSONDecodeError):
                    self.seen = set()
            self.url="https://www.threads.net/@"
            self.QUERY_SELECTOR = "script[type='application/json'][data-sjs]"
            self.search_choice=search_choice
            self.acc=acc
            self.gclike=0 #greater like counts
            self.gcreply=0 #greater reply counts
            self.lttext=0 #less than text counts
            self.image_retrieve=False
            self.username=username
        def filter_setting(self,gclike:int=0, gcreply:int=0, lttext:int=0, image_retrieve:bool=False):
            self.gclike=gclike if gclike>0 else 0
            self.gcreply=gcreply if gcreply>0 else 0
            self.lttext=lttext if lttext>0 else 0
            self.image_retrieve= True  if image_retrieve else False
        def _save_seen(self):
            with open('config/existID.json', 'w', encoding='utf-8') as f:
                json.dump(list(self.seen), f, ensure_ascii=False, indent=1)
            print("seen ID saved.")
        def _parse_post(self,item: Dict) -> Dict:
            """æŠŠåŸå§‹ thread_items ç‰©ä»¶æ¿ƒç¸®æˆç²¾ç°¡æ¬„ä½ã€‚"""
            return jmespath.search(
                """
                {
                id: post.id,
                code: post.code,
                text: post.caption.text,
                like_count: post.like_count,
                reply_count: post.text_post_app_info.direct_reply_count,
                username: post.user.username,
                timestamp: post.taken_at
                }
                """,
                item,
            )
        def _sort_posts(self,posts: List[Dict], sort_key: str, ascending: bool = False) -> List[Dict]:
            """æ ¹æ“š sort_key å°è²¼æ–‡åˆ—è¡¨æ’åºï¼Œé è¨­ by sort_key é™å†ªã€‚"""
            # å¦‚æœ key ä¸å­˜åœ¨ï¼Œé è¨­å€¼ç‚º 0 æˆ–ç©ºå­—ä¸²
            return sorted(
                posts,
                key=lambda p: p.get(sort_key) or 0,
                reverse=not ascending,
        )
        async def Top_crawl(self,batch:int=3)-> List[Dict]:
            async with async_playwright() as p:
                browser = await p.chromium.launch(headless=True)
                context = await browser.new_context()
                page = await context.new_page()
                usertmp=self.username
                posts: List[Dict] = []
                for idx,username in enumerate(self.username):
                    url=self.url+username
                    await page.goto(url, timeout=60000)
                    # ç­‰å¾…éš±è— JSON æ›é€² DOM
                    await page.wait_for_selector(self.QUERY_SELECTOR, state="attached", timeout=60000)
                    selector = Selector(await page.content())
                    scripts = selector.css(f"{self.QUERY_SELECTOR}::text").getall()
                    done=False
                    pre=len(posts)
                    for payload in scripts:
                        data = json.loads(payload)
                        # ç¬¬ä¸€æ¬¡å˜—è©¦æŠ½å‡ºç¸½è²¼æ–‡æ•¸ï¼ˆJSON ä¸­å¯èƒ½å‡ºç¾ user.media_count æˆ– thread_countï¼‰
                        if '"thread_items"' not in payload:
                            continue
                        for group in nested_lookup("thread_items", data):
                            for item in group:
                                parsed = self._parse_post(item)
                                if self.gclike and int(parsed.get("like_count", 0)) < self.gclike:
                                    continue
                                if self.gcreply and int(parsed.get("reply_count", 0)) < self.gcreply:
                                    continue
                                if self.lttext and len(parsed.get("text", "") or "") > self.lttext:
                                    continue
                                if self.image_retrieve == parsed.get("media_urls"):
                                    continue
                                print(f"post id: {parsed['id']}")
                                if parsed["id"] not in self.seen:
                                    posts.append(parsed)
                                    self.seen.add(parsed["id"])
                                    # print(self.seen)
                                    if len(posts) >= (idx+1)*batch: 
                                        break
                            if len(posts) >= (idx+1)*batch: 
                                done=True
                                break
                        if done:
                            break
                    if len(posts) == pre:
                        usertmp.remove(username)
                self.username=usertmp
                posts=self._sort_posts(posts,self.search_choice,self.acc)
                self._save_seen()
                print(len(posts))
                return posts    
        def printPost(self,posts):
            for idx, post in enumerate(posts, 1):
                excerpt = (post["text"] or "").replace("\n", " ")[:150]
                url = f"https://www.threads.net/@{post['username']}/post/{post['code']}"
                print(f"{idx}. {excerpt}â€¦")
                print(f"   ğŸ‘ {post['like_count']}   ğŸ’¬ {post['reply_count']}   {url}\n")
        def printJosn(self,posts):
            out={"post":posts}
            print(json.dumps(out, ensure_ascii=False, indent=1))
        def getJosn(self,posts):
            """å°‡è²¼æ–‡è½‰æ›ç‚º JSON æ ¼å¼"""
            cleaned_posts = []
            for post in posts:
                # å‰µå»ºä¸€å€‹åªåŒ…å«æ‰€éœ€æ¬„ä½çš„æ–°å­—å…¸
                cleaned_post = {
                    "id": post.get("id", ""),
                    "username": post.get("username", ""), # å°æ‡‰ Google Sheets çš„ User_id
                    "text": (post.get("text", "") or "").replace("\n", " "), # è™•ç†æ–‡å­—å…§å®¹ï¼Œæ›¿æ›æ›è¡Œç¬¦
                    "like_count": post.get("like_count", 0),
                    "reply_count": post.get("reply_count", 0),
                    "timestamp":post.get("timestamp", 0),
                }
                cleaned_posts.append(cleaned_post)
            out = {"posts": cleaned_posts}
            return json.dumps(out, ensure_ascii=False, indent=1)
if __name__ == "__main__":
    with open('config/threadsUser.json','r',encoding='utf-8') as f:
        cfg = json.load(f)
    threads=Threads_scraper(username=["huang.weizhu"])
    threads.filter_setting(gclike=1)
    posts=(asyncio.run(
            threads.Top_crawl(batch=10)
        ))
    #threads.printPost(posts)
    p=json.loads(threads.getJosn(posts))
    print(p)