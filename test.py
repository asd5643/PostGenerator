import asyncio, json, os
from typing import List, Dict, Optional
from nested_lookup import nested_lookup
from parsel import Selector        # ä»å¯ç”¨ä¾†æŠ“ç¬¬ä¸€æ¬¡çš„ script
from playwright.async_api import async_playwright

class ThreadsScraper:
    def __init__(self,
                 search_choice: str = "like_count",
                 acc: bool = False,
                 username: Optional[List[str]] = None):
        self.url = "https://www.threads.net/@"
        self.search_choice = search_choice
        self.acc = acc
        self.username = username or []
        self.seen = self._load_seen()

    # ---------- å…¬ç”¨å°å·¥å…· ----------
    def _load_seen(self):
        try:
            with open("config/existID.json", "r", encoding="utf-8") as f:
                return set(json.load(f))
        except (FileNotFoundError, json.JSONDecodeError):
            return set()

    def _save_seen(self):
        with open("config/existID.json", "w", encoding="utf-8") as f:
            json.dump(list(self.seen), f, ensure_ascii=False, indent=1)

    def _parse_post(self, item: Dict) -> Dict:
        """ç”¨ jmespath æ¿¾æ‰è‚¥è‚‰ã€‚"""
        import jmespath
        return jmespath.search(
            """
            {
              id: post.id,
              code: post.code,
              text: post.caption.text,
              like_count: post.like_count,
              reply_count: post.text_post_app_info.direct_reply_count,
              username: post.user.username,
              media_urls: post.carousel_media[].image_versions2.candidates[0].url,
              video_urls: post.video_versions[].url
            }
            """,
            item,
        )

    # ---------- ğŸ‘‡ æ–°çš„æŠ“æ³• ----------
    async def crawl_user(self, username: str, max_posts: int = 20) -> List[Dict]:
        """å›å‚³å–®ä¸€ä½¿ç”¨è€…çš„è²¼æ–‡ï¼ˆå·²æ’åºã€å»é‡ï¼‰ã€‚"""
        posts: List[Dict] = []

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=True)
            context = await browser.new_context()
            page = await context.new_page()

            # æ””æˆª /api/graphql çš„ response
            async def handle_response(resp):
                if "/api/graphql" not in resp.url:
                    return
                ctype = resp.headers.get("content-type", "")
                if "json" not in ctype:            # åªè¦ JSON å›æ‡‰
                    return
                try:
                    body = await resp.json()
                except Exception:
                    return

                # æ‰¾æ‰€æœ‰ thread_items
                for group in nested_lookup("thread_items", body):
                    for item in group:
                        parsed = self._parse_post(item)
                        pid = parsed["id"]
                        if pid in self.seen:
                            continue
                        posts.append(parsed)
                        self.seen.add(pid)
              
            page.on("response", handle_response)
            while len(posts) < max_posts:
                # é€²å…¥é é¢ï¼ˆæŠŠç¬¬ä¸€æ‰¹é¦–åˆ·è³‡æ–™ä¹Ÿåƒæ‰ï¼‰
                await page.goto(f"{self.url}{username}", timeout=60_000)

                # æŠŠ hydration script è£çš„ thread_items ä¹Ÿæ’ˆä¸€ä¸‹
                selector = Selector(await page.content())
            
                for payload in selector.css("script[type='application/json'][data-sjs]::text").getall():
                    if '"thread_items"' not in payload:
                        continue
                    data = json.loads(payload)
                    for group in nested_lookup("thread_items", data):
                        print("ok1")
                        for item in group:
                            print("ok2")
                            if(len(posts)>=max_posts):
                                print("finish")
                            parsed = self._parse_post(item)
                            pid = parsed["id"]
                            if pid not in self.seen:
                                print("now:",len(posts))
                                posts.append(parsed)
                                self.seen.add(pid)

                # è§¸ç™¼æ²å‹•ä»¥ä¾¿å‰ç«¯é€åˆ†é  GraphQL
                print("scrolling...:", len(posts))
                await page.mouse.wheel(0, 8000)              # ä¸€æ¬¡æ»‘åˆ°åº•
                await page.wait_for_timeout(20000)            # ç°¡å–®ç­‰å¾…ç¶²è·¯éœæ­¢
            await page.close()
            await browser.close()

        self._save_seen()
        # ä¾éœ€æ±‚æ’åº
        posts.sort(key=lambda p: p.get(self.search_choice) or 0,
                   reverse=not self.acc)
        return posts[:max_posts]

# --- demo ---
async def demo():
    s = ThreadsScraper(username=["huang.weizhu"])
    result = await s.crawl_user("huang.weizhu", max_posts=30)
    print(json.dumps(result, ensure_ascii=False, indent=2))

if __name__ == "__main__":
    asyncio.run(demo())